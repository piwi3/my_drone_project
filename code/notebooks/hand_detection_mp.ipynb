{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# For modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091da9a0",
   "metadata": {},
   "source": [
    "# Intro: Keypoints using holisitc mediapipe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad10b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaca02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_coords(results):\n",
    "    try:\n",
    "        x_min = min(res.x for res in results.right_hand_landmarks.landmark)\n",
    "        y_min = min(res.y for res in results.right_hand_landmarks.landmark)\n",
    "        x_max = max(res.x for res in results.right_hand_landmarks.landmark)\n",
    "        y_max = max(res.y for res in results.right_hand_landmarks.landmark)\n",
    "        return x_min, y_min, x_max, y_max\n",
    "    except: \n",
    "        return 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(image, results):\n",
    "    coords = get_bbox_coords(results)\n",
    "    #print(tuple(np.multiply(coords, [1920, 1080, 1920, 1080]).astype(int)))\n",
    "    cv2.rectangle(image,\n",
    "                  tuple(np.multiply(coords[:2], [1920, 1080]).astype(int)),\n",
    "                  tuple(np.multiply(coords[2:], [1920, 1080]).astype(int)),\n",
    "                  (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87013e84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# Initiate holistic model\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "while cap.isOpened():\n",
    "    # Read frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Make Detections\n",
    "    image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "    # Draw face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "    draw_landmarks(image, results)\n",
    "\n",
    "    # Draw bbox\n",
    "    draw_bbox(image, results)\n",
    "\n",
    "    # Show to screen\n",
    "    cv2.imshow('Video Feed', image)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    # Break program\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee7d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "cv2.startWindowThread()\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Make Detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Draw face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # Draw bbox\n",
    "        draw_bbox(image, results)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('Video Feed', image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        # Break program\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb38a31",
   "metadata": {},
   "source": [
    "# Define function to extract keypoint values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract all values for right hand only!\n",
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Extracts key points from results vector delivered by holistic medipipe model.\n",
    "    \"\"\"\n",
    "    rhand = np.zeros(21*3)\n",
    "    if results.right_hand_landmarks:\n",
    "        rhand = np.array([[result.x, result.y, result.z] \n",
    "                          for result in results.right_hand_landmarks.landmark]).flatten()\n",
    "        \n",
    "    return rhand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape\n",
    "extract_keypoints(results).shape, 21 * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb05c4",
   "metadata": {},
   "source": [
    "# Setup folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc3cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['fist', 'palm', 'index', 'ok', 'thumb_up']\n",
    "imgs_per_action = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../../data/hand_detection_mp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing files\n",
    "#for action in actions:\n",
    " #   for file in os.listdir(os.path.join(base_path, action)):\n",
    "  #      os.remove(os.path.join(base_path, action, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7095e0",
   "metadata": {},
   "source": [
    "# Generate data for hand gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take pictures for each ction and save landmarks as numpy array\n",
    "cap = cv2.VideoCapture(1)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    for action in actions:\n",
    "        for img_num in range(imgs_per_action):\n",
    "            \n",
    "            # Read frame\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make Detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            # Draw face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "            draw_landmarks(image, results)\n",
    "\n",
    "            # Apply wait logic\n",
    "            if img_num == 0: \n",
    "                cv2.putText(image, 'STARTING COLLECTION', (100,200), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, f'Collecting frames for {action}', (15,50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(3000)\n",
    "            else: \n",
    "                cv2.putText(image, f'Collecting frames for {action}, ({img_num}/{imgs_per_action})', (15,50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(1000)\n",
    "\n",
    "            # Export keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            npy_path = os.path.join(base_path, action, f'keypoints_{action}_{img_num}')\n",
    "            np.save(npy_path, keypoints)\n",
    "\n",
    "\n",
    "\n",
    "            # Break program\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        # Break program\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce79ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dda37",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9747451",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {action:num for num, action in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a11d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = [], []\n",
    "for action in actions:\n",
    "    for file in os.listdir(os.path.join(base_path, action)):\n",
    "        res = np.load(os.path.join(base_path, action, file))\n",
    "        if all(res != np.zeros(21*3)):\n",
    "            images.append(res)\n",
    "            labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape, len(images), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(images)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb89ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33277ec",
   "metadata": {},
   "source": [
    "# Create neuronal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ee3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(63,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(actions), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1223d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f796e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_split = 0.1, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4799bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['categorical_accuracy'], label = 'train_data')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label = 'validation_data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a4abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label = 'train_data')\n",
    "plt.plot(history.history['val_loss'], label = 'validation_data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741feab9",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e27ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show score for train and test data\n",
    "model.evaluate(X_train, y_train), model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine predictions for test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true=np.argmax(y_test, axis=1), y_pred=np.argmax(y_pred, axis=1))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=actions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "disp.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('hand_gesture_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe89648",
   "metadata": {},
   "source": [
    "# Realtime gesture detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from h5 file hand_detection_model.ipynbre for testing purposes)\n",
    "model = load_model('hand_gesture_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafedba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# Initialize variables\n",
    "datetime_prv = datetime.now() - timedelta(seconds=1)\n",
    "class_prob = 0\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Make Detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Draw face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # NEW predict hand gesture\n",
    "        if datetime.now() >= datetime_prv + timedelta(seconds=0.5) and results.right_hand_landmarks:\n",
    "            img = image\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            keypoints = extract_keypoints(results) \n",
    "            keypoints = keypoints.reshape(1,-1)\n",
    "            y_pred = model.predict(keypoints)\n",
    "            class_id = np.argmax(y_pred)\n",
    "            class_prob = np.max(y_pred)\n",
    "            coords = get_bbox_coords(results)\n",
    "            \n",
    "        # NEW draw handgesture if detetced\n",
    "        if class_prob > 0.5 and results.right_hand_landmarks:\n",
    "        \n",
    "            # Draw bbox around hand\n",
    "            draw_bbox(image, results)\n",
    "\n",
    "            # Draw label box\n",
    "            coords = get_bbox_coords(results)\n",
    "            cv2.rectangle(image,\n",
    "                          tuple(np.add(np.multiply(coords[:2], [1920, 1080]).astype(int), [0, -30])),\n",
    "                          tuple(np.add(np.multiply(coords[:2], [1920, 1080]).astype(int), [80, 0])),\n",
    "                          (0,0,255), -1)\n",
    "\n",
    "            # Put text in label\n",
    "            cv2.putText(image, f'{actions[class_id]}', \n",
    "                        tuple(np.add(np.multiply(coords[:2], [1920, 1080]).astype(int), [0, -5])),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('Video Feed', image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        # Break program\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5eec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfaa5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that wraps everything for drone operations\n",
    "def model_full(image, model, res):\n",
    "    \n",
    "    # Predict hand gesture, i.d. determine class_id with max proba and probability \n",
    "    image, results = mediapipe_detection(frame, model)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    keypoints = extract_keypoints(results)\n",
    "    keypoints = keypoints.reshape(1,-1)\n",
    "    \n",
    "    y_pred = model.predict(keypoints)\n",
    "    class_id = np.argmax(y_pred)\n",
    "    class_prob = np.max(y_pred)\n",
    "    \n",
    "    \n",
    "    # Calculate bbox params -> center_point and area for tracking\n",
    "    coords = get_bbox_coords(results)\n",
    "    coords = tuple(np.multiply(coords, [res[0], res[1], res[0], res[1]]).astype(int))\n",
    "    \n",
    "    center = (coords[0] + coords[2]) // 2, (coords[1] + coords[3]) // 2\n",
    "    area = (coords[2] - coords[0]) * (coords[3] - coords[1]) // 1\n",
    "    \n",
    "    return (class_id, class_prob), (center, area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1667a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4389d3574c8dc0acd3d2352aef74950413868cf79a3d29b0accd92f94082a3a4"
  },
  "kernelspec": {
   "display_name": "Python (tello_ai)",
   "language": "python",
   "name": "conda-env-tello_ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
